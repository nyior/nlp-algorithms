{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c729f062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nyiorclement/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# While stemming is a common technique in NLP, it is not always the best choice for every task.\n",
    "# It is often used in conjunction with other techniques, such as lemmatization, to improve\n",
    "# the accuracy of text analysis.\n",
    "# lemmatization is a more advanced technique that takes into account the context of the word\n",
    "# and its meaning, rather than just removing suffixes.\n",
    "# This can lead to more accurate results, especially for tasks such as sentiment analysis or text classification\n",
    "# where the meaning of the word is important.\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1431f5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting : connecting\n",
      "connected : connected\n",
      "connectivity : connectivity\n",
      "connect : connect\n",
      "connects : connects\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "connect_tokens = ['connecting', 'connected', 'connectivity', 'connect', 'connects']\n",
    "# lemmatizing the tokens\n",
    "\n",
    "for token in connect_tokens:\n",
    "    print(f\"{token} : {lemmatizer.lemmatize(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6047a0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned : learned\n",
      "learning : learning\n",
      "learn : learn\n",
      "learns : learns\n",
      "learner : learner\n",
      "learners : learner\n"
     ]
    }
   ],
   "source": [
    "learn_tokens = ['learned', 'learning', 'learn', 'learns', 'learner', 'learners']\n",
    "\n",
    "for token in learn_tokens:\n",
    "    print(f\"{token} : {lemmatizer.lemmatize(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d93e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes : like\n",
      "better : better\n",
      "worse : worse\n"
     ]
    }
   ],
   "source": [
    "likes_tokens = ['likes', 'better', 'worse']\n",
    "for token in likes_tokens:\n",
    "    print(f\"{token} : {lemmatizer.lemmatize(token)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bank-app-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
