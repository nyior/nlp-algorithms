{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c882327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32383bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9979894161224365}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_classifier = pipeline(\"sentiment-analysis\")\n",
    "sentiment_classifier(\"I'm more excited to be learning about large language models \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d7cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1340ea2e471b4a12bdd28bb1d31901d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453a2fc133d04c4d9e3f4d65a7b57a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4effc0649f46c4bf7f00cc54f05b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f9f55ecab342fbb599e54d61dabcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933562d3aaf1456288a18fe76ba2eae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfb8e2526514b5aa36a8e484fa466ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/nyiorclement/development/v-envs/bank-app-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.9955586,\n",
       "  'index': 4,\n",
       "  'word': 'Anna',\n",
       "  'start': 12,\n",
       "  'end': 16},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9994766,\n",
       "  'index': 9,\n",
       "  'word': 'New',\n",
       "  'start': 34,\n",
       "  'end': 37},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9992022,\n",
       "  'index': 10,\n",
       "  'word': 'York',\n",
       "  'start': 38,\n",
       "  'end': 42},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.99269027,\n",
       "  'index': 13,\n",
       "  'word': 'Morgan',\n",
       "  'start': 52,\n",
       "  'end': 58},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99723876,\n",
       "  'index': 14,\n",
       "  'word': 'Stanley',\n",
       "  'start': 59,\n",
       "  'end': 66}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "ner(\"Her name is Anna and she works in New York city for Morgan Stanley\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e43b824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a9e759d8334eb4abe4efc56071df8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652567a310e7467ebae2f8c7a920f273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6cde3510174e74b36fcb7951566977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f337878760945a493439d50e6008e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3014500e1c4bf687dee43a03bafc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870b4ed39a354712b7fe5eac49919420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.9938650727272034, 0.003273784415796399, 0.0028610362205654383]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "sequence = \"one day I will see the world\"\n",
    "labels = [\"travel\", \"cooking\", \"dancing\"]\n",
    "\n",
    "zeroshot_classifier(sequence, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa9d4d",
   "metadata": {},
   "source": [
    "## Pre-trained Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e615c762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352e9151ff8b4e38a43034ce5852fdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c96157b3bdd41fea1744e8541af48c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aec55c626f24d24a13ec9df4a15a9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31808b587e8490ba22f614792c671eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 1049, 2062, 7568, 2000, 2022, 4083, 2055, 2312, 2653, 4275, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "sentence = \"I'm more excited to be learning about large language models \"\n",
    "input_ids = tokenizer(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993ff686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'more', 'excited', 'to', 'be', 'learning', 'about', 'large', 'language', 'models']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6c0b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 1049, 2062, 7568, 2000, 2022, 4083, 2055, 2312, 2653, 4275]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa08b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ' m more excited to be learning about large language models\n"
     ]
    }
   ],
   "source": [
    "decoded_ids = tokenizer.decode(token_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "041f14df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Special tokens beginning and end\n",
    "tokenizer.decode(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224945a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(102)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82bdf7",
   "metadata": {},
   "source": [
    "### How xlnet does tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242e9b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28dcdc10ada45c2ad968df0033c721f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194e7c670ac446e4a845a2f9774e9757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0474129ed342ed8dc68a4b40663c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [35, 26, 98, 70, 5564, 22, 39, 1899, 75, 392, 1243, 2626, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model2 = \"xlnet-base-cased\"\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model2)\n",
    "\n",
    "input_ids = tokenizer2(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7de7b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', \"'\", 'm', '▁more', '▁excited', '▁to', '▁be', '▁learning', '▁about', '▁large', '▁language', '▁models']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer2.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95813c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer2.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c53a0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sep>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Again, two special tokens, but added at the end\n",
    "tokenizer2.decode(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c0850",
   "metadata": {},
   "source": [
    "## Special tokens\n",
    "\n",
    "Special Tokens in Large Language Models\n",
    "Special tokens are a fundamental concept in the world of large language models. They are specific placeholders or markers that help the model perform various tasks and handle specific instructions within the text. These tokens act like signposts or markers that assist the model in understanding the structure and context of the input text, guiding its behavior across various tasks. This ensures that the output of the tokenization is in a format that your model of choice will understand.\n",
    "\n",
    "Let's discuss some key types of special tokens and their purposes.\n",
    "\n",
    "#### CLS and SEP Tokens\n",
    "\n",
    "The first are the CLS and SEP tags. The CLS tag stands for classification, while SEP stands for separator. These special tokens are commonly used in tasks like text classification and sentence pair classification. In a typical scenario, you might have a sentence or text passage and want the model to classify it into a category or determine its relationship with another sentence. The CLS token is usually placed at the beginning of the input, and the SEP token is used to separate different segments of text.\n",
    "\n",
    "#### Masked Token\n",
    "\n",
    "The masked token is used in tasks related to masked automatic language modeling or text generation with a blank to fill in. For example, you might provide a sentence with a word replaced by a mask, and the model's task is to predict the missing word. This is commonly used in tasks like text completion.\n",
    "\n",
    "#### Custom Special Tokens\n",
    "\n",
    "Depending on the specific task you want the model to perform, you may introduce custom special tokens. For instance, if you are using the model for translation, you might have special tokens like <source> and <target> to indicate the source and target languages, respectively. These tokens help guide the model's behavior during the translation process.\n",
    "\n",
    "#### Padding and Truncation Tokens\n",
    "\n",
    "When feeding multiple sentences of varying lengths into the model, you may need special tokens for padding, which involves adding extra tokens to ensure all inputs are the same length. Alternatively, truncation is used to shorten longer inputs to a specific length. These tokens ensure that the inputs are consistent and align with the model's architecture.\n",
    "\n",
    "#### Model-Specific Requirements\n",
    "\n",
    "Each model has different requirements and uses when it comes to special tokens. Therefore, it is important to specify which model you are going to use when working with the tokenizer. This ensures that the tokenizer processes the text in the correct format for that specific model.\n",
    "\n",
    "#### Key Takeaways\n",
    "- Special tokens serve as placeholders or markers that guide large language models in understanding input structure and context.\n",
    "- CLS (classification) and SEP (separator) tokens are essential for tasks like text classification and sentence pair classification.\n",
    "- Masked tokens enable models to predict missing words, commonly used in masked language modeling and text completion.\n",
    "- Custom special tokens can be introduced for specific tasks such as translation to indicate source and target languages.\n",
    "- Padding and truncation tokens ensure input sequences are consistent in length, aligning with model architecture requirements.\n",
    "- Specifying the model when using a tokenizer is crucial to ensure correct text processing and formatting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682c0fc",
   "metadata": {},
   "source": [
    "## Huggingface and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38f9e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm more excited to be learning about large language models \n",
      "{'input_ids': [35, 26, 98, 70, 5564, 22, 39, 1899, 75, 392, 1243, 2626, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "print(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5975767b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76026109e0b4176b989278e55b9d462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70acc5b46d5444f19cc26aecbff925cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2896b8ec7b2b4ab4ac72a330c202ce3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 1005, 1049, 2062, 7568, 2000, 2022, 4083, 2055, 2312, 2653,\n",
      "         4275,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "input_ids_pt = tokenizer(sentence, return_tensors=\"pt\")\n",
    "print(input_ids_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47f2bc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd0066d4b0f4ca286a52fd60e0ba5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**input_ids_pt).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bank-app-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
